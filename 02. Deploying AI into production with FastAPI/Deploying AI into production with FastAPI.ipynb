{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2843d105-0a25-4c9f-9b55-4c9677e072ad",
   "metadata": {},
   "source": [
    "# Deploying AI into production with FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed57a5-f8e8-4c94-b6f9-b9f3190e4e6d",
   "metadata": {},
   "source": [
    "## Chapter 1 - Introducion to FastAPI for Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da86eb5-b9a1-4f6d-8dac-4e3113a094f9",
   "metadata": {},
   "source": [
    "### Section 1.1 - GET and POST requests for AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f1fc9-dd5c-4d65-a6bd-6a1237e427a1",
   "metadata": {},
   "source": [
    "#### GET endpoint for model information\n",
    "\n",
    "You're part of a machine learning team that has developed several machine learning models, each designed for different tasks such as sentiment analysis, product categorization, and customer churn prediction. You're working on deploying these models, and you need to create an endpoint that provides basic information about each model.\n",
    "\n",
    "Your task is to implement a GET endpoint at route `/model-info/{model_id}` that retrieves and returns this essential model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49366a73-f3f2-46e0-83cd-f098c5fd40ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Add model_id as a path parameter in the route\n",
    "@app.get(\"/model-info/{model_id}\")\n",
    "# Pass on the model id as an argument\n",
    "async def get_model_info(model_id: int):\n",
    "    # Check if the passed model id is 0\n",
    "    if model_id == 0:\n",
    "      \t# Raise the right status code for not found\n",
    "        raise HTTPException(status_code=404, detail=\"Model not found\")\n",
    "    model_info = get_model_details(id)  \n",
    "    # Return the model id and info in the dict\n",
    "    return {\"model_id\": model_id, \"model_name\": model_info}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97aab6-7399-4def-a802-0d1e8824fcdd",
   "metadata": {},
   "source": [
    "#### POST endpoint for model registration\n",
    "\n",
    "While the GET endpoint you created earlier allows users to retrieve information about existing models, you now need a way for authorized team members to register new models or update information about existing ones.\n",
    "\n",
    "You need to create a POST endpoint that allows team members to register new models or update existing ones. This endpoint will store model information on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c85e3df6-5259-4470-a287-de2fb5530746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from pydantic import BaseModel\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "model_db = {}\n",
    "\n",
    "class ModelInfo(BaseModel):\n",
    "    model_id: int\n",
    "    model_name: str\n",
    "    description: str\n",
    "\n",
    "# Specify the status code for successful POST request\n",
    "@app.post(\"/register-model\", status_code=201)\n",
    "# Pass the model info from the request as function parameter \n",
    "def register_model(model_info: ModelInfo):\n",
    "    # Add new model's information dictionary to the model database\n",
    "    model_db[model_info.model_id] = model_info.model_dump()\n",
    "    # Return model info dictionary corresponding to model along with success status code\n",
    "    return {\"message\": \"Model registered successfully\", \"model\": model_info}, 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7714a249-fe00-4dde-bad4-d5ebf037dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message': 'Model registered successfully', 'model': {'model_id': 1, 'model_name': 'cnn', 'description': 'convolutional nn'}}, 201]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "data = {\n",
    "    \"model_id\":1, \n",
    "    \"model_name\": \"cnn\", \n",
    "    \"description\": \"convolutional nn\"}\n",
    "\n",
    "url = \"http://localhost:8000/register-model\"\n",
    "headers = {\"Content_Type\": \"Application-json\"}\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c78b6-a13a-404e-a573-5201ec5c31ea",
   "metadata": {},
   "source": [
    "### Section 1.2 - FastAPI prediction with a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302abcba-a153-4fe6-ba9a-6d087fb64eec",
   "metadata": {},
   "source": [
    "#### Preperation\n",
    "\n",
    "The model from the exercises does not work. So hwere we quickly trains and save our own model that can be used for the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40ee3fda-738c-4545-a98f-0421bf2a24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"SIH/palmer-penguins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f30acb74-0d6b-410a-9e87-1dcdb5897988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  year  \n",
       "0       3750.0    male  2007  \n",
       "1       3800.0  female  2007  \n",
       "2       3250.0  female  2007  \n",
       "3          NaN    None  2007  \n",
       "4       3450.0  female  2007  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ds['train'].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6afbf68-a3bb-41c9-9bcc-3b3c11c89ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy scores: [0.98181818 1.         1.         0.98148148 1.        ]\n",
      "Mean CV accuracy: 0.9926599326599327\n",
      "Test set accuracy: 0.9855072463768116\n",
      "Loaded model type: <class 'sklearn.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Only include numerical features + target\n",
    "features = ['species', 'bill_length_mm', 'bill_depth_mm',\n",
    "            'flipper_length_mm', 'body_mass_g']\n",
    "\n",
    "# 0. Prepare the dataset\n",
    "df_dataset = df[features].dropna()\n",
    "X = df_dataset.drop(\"species\", axis=1)\n",
    "y = df_dataset[\"species\"]\n",
    "\n",
    "# 1. Split first\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Define pipeline (scaler + classifier)\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "# 3. Perform cross-validation only on the training set\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5)\n",
    "\n",
    "# 4. Report CV accuracy\n",
    "print(\"Cross-validation accuracy scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# 5. Fit the pipeline on the full training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate on the test set\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"Test set accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# 7. Save the pipeline (includes scaler + model)\n",
    "joblib.dump(pipe, \"penguin_classifier.pkl\")\n",
    "\n",
    "# 8. Load the pipeline\n",
    "model = joblib.load(\"penguin_classifier.pkl\")\n",
    "print(\"Loaded model type:\", type(model))\n",
    "\n",
    "# 9. Use the loaded model for prediction\n",
    "# (Optional — uncomment to test)\n",
    "# y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72669f6d-740c-4a03-8f79-aba10b038ee4",
   "metadata": {},
   "source": [
    "#### Load the pre-trained model\n",
    "\n",
    "You're a data scientist at an animal conservation company. You've been given a pre-trained machine learning model that predicts penguin species.\n",
    "\n",
    "Your task is to load this model so it can be used in an API. The model has been saved using `joblib`.\n",
    "\n",
    "A pre-trained ML model is stored in the pickle file: `penguin_classifier.pkl`\n",
    "\n",
    "Write a script to load the pickle file as a model. Test your script by running `python3 solution.py` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77789f78-3e7d-4afc-baad-ebe1576f37b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model type: <class 'sklearn.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary module\n",
    "import joblib\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = joblib.load('penguin_classifier.pkl')\n",
    "\n",
    "# Print the type of the loaded model\n",
    "print(f\"Loaded model type: {type(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870e446-3092-4d5a-9595-9c28438637c7",
   "metadata": {},
   "source": [
    "#### Create the prediction endpoint\n",
    "\n",
    "In this exercise, you'll create a prediction endpoint that uses a pre-trained model to estimate diabetes progression.\n",
    "\n",
    "The model has been trained on a dataset which has three features age, bmi and blood_pressure. It then predicts the diabetes progression score. Using these inputs, it predicts a diabetes progression score, which helps assess how the condition may develop over time.\n",
    "\n",
    "You'll use FastAPI to create a POST endpoint that accepts patient data and returns a prediction of diabetes progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7222cd36-5aa8-4bd1-a522-2ca80129a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = joblib.load('penguin_classifier.pkl')\n",
    "\n",
    "# Print the type of the loaded model\n",
    "print(f\"Loaded model type: {type(model)}\")\n",
    "class PengiunFeatures(BaseModel):\n",
    "    bill_length_mm: float\n",
    "    bill_depth_mm: float\n",
    "    flipper_length_mm: float\n",
    "    body_mass_g: float\n",
    "    \n",
    "# Create FastAPI instance\n",
    "app = FastAPI()\n",
    "\n",
    "# # Create a POST request endpoint at the route \"/predict\"\n",
    "@app.post(\"/predict\")\n",
    "async def predict_progression(features: PengiunFeatures):\n",
    "    input_data = pd.DataFrame([features.model_dump()])\n",
    "    \n",
    "    prediction = model.predict(input_data)\n",
    "    return {\"predicted_progression\": prediction[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10fe37eb-8b31-4c08-8f5e-a362e866a924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicted_progression': 'Adelie'}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import requests\n",
    "\n",
    "class PengiunFeatures(BaseModel):\n",
    "    bill_length_mm: float\n",
    "    bill_depth_mm: float\n",
    "    flipper_length_mm: float\n",
    "    body_mass_g: float\n",
    "\n",
    "\n",
    "pengiun = PengiunFeatures(bill_length_mm=39.1,\n",
    "                          bill_depth_mm=18.7,\n",
    "                          flipper_length_mm=181.0,\n",
    "                          body_mass_g=3750.0)\n",
    "\n",
    "url = \"http://localhost:8000/predict\"\n",
    "data = pengiun.model_dump()\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b096eefb-5e71-4963-a5e5-b5a8100b8bcb",
   "metadata": {},
   "source": [
    "#### Running the FastAPI app\n",
    "\n",
    "Your FastAPI app has been saved in a Python file called `main.py`. You would like to run the app from a Python script using uvicorn.\n",
    "\n",
    "To serve your FastAPI app directly via the Python script in `solution.py`, you need to finish adding the code block that sets up the host and port of the server where the API will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6926a360-cc7a-4b75-ba93-5196ab2572ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting solution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile solution.py\n",
    "# Import the server module\n",
    "import uvicorn\n",
    "from main import app\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Start the uvicorn server\n",
    "    uvicorn.run(\n",
    "\t  app, \n",
    "      # Configure the host\n",
    "      host=\"0.0.0.0\",\n",
    "      # Configure the port\n",
    "      port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb1087-0dd8-4f32-85e5-3ab9e7ca371b",
   "metadata": {},
   "source": [
    "with the snippet below we can start the server using python. It is best to do this in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100785d6-0b82-491f-ba2b-3ec4b8880d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4e927-ed17-47d6-b0c0-2cfeb0d81963",
   "metadata": {},
   "source": [
    "now post to the server on port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d21845a-e1cf-4655-a4d0-904facf652c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicted_progression': 'Adelie'}\n"
     ]
    }
   ],
   "source": [
    "pengiun = PengiunFeatures(bill_length_mm=39.1,\n",
    "                          bill_depth_mm=18.7,\n",
    "                          flipper_length_mm=181.0,\n",
    "                          body_mass_g=3750.0)\n",
    "\n",
    "url = \"http://localhost:8080/predict\"\n",
    "data = pengiun.model_dump()\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433b739-3564-48d5-8249-6006c347bd71",
   "metadata": {},
   "source": [
    "### Section 1.3 - Create a PYdantic model for ML input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86012f62-44b8-4d32-adc1-b1eb07ae9e17",
   "metadata": {},
   "source": [
    "#### Create a Pydantic model for ML input\n",
    "\n",
    "You're developing a FastAPI application to deploy a machine learning model that predicts the quality score of coffee based on attributes including aroma, flavor, and altitude.\n",
    "\n",
    "The first step is to create a Pydantic model to validate the input request data for your ML model and ensure that only valid data flows through the model for successful model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "184b18be-ca8a-4379-aed3-7054407b6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the base class from pydantic\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class CoffeeQualityInput(BaseModel):\n",
    "    # Use apt data type for each attribute of coffee quality\n",
    "    aroma: float  \n",
    "    flavor: float  \n",
    "    altitude: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40da73c-caef-43a0-ba6a-add1198ca38f",
   "metadata": {},
   "source": [
    "#### Validate request and response for ML prediction\n",
    "\n",
    "Building on your work as a data scientist at the coffee company, you now need to create a FastAPI endpoint that validates input request using `CoffeeQualityInput` data validation model and a `QualityPrediction` for response validation.\n",
    "\n",
    "This endpoint will accept coffee data and return a quality prediction along with the confidence score.\n",
    "\n",
    "The model is already loaded into a function called `predict_quality` for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c306f479-068d-4a42-8ff3-51c6193034d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoffeeQualityInput(BaseModel):\n",
    "    aroma: float\n",
    "    flavor: float\n",
    "    altitude: int\n",
    "    \n",
    "class QualityPrediction(BaseModel):\n",
    "    quality_score: float \n",
    "    confidence: float\n",
    "\n",
    "# Specify the data model to validate response\n",
    "@app.post(\"/predict\", response_model=QualityPrediction) \n",
    "# Specify the data model to validate input request\n",
    "def predict(coffee_data: CoffeeQualityInput):\n",
    "    prediction = predict_quality(coffee_data)\n",
    "    return prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22377c-6ad9-490d-a20b-1e12b3fd977f",
   "metadata": {},
   "source": [
    "## Chapter 2 - Integrating AI Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846493b9-0f2e-429f-ac64-ab8bfb79c770",
   "metadata": {},
   "source": [
    "#### Handle textual request data\n",
    "\n",
    "Another requirement in the content moderation system is to take into account user comments' sentiment. The system needs to identify specific problematic phrases to help moderators review potentially inappropriate content.\n",
    "\n",
    "You'll create an endpoint that analyzes text coming from users and extracts standardized moderation flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3c1f161-7b16-44ea-95b0-25a2281a9f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/analyze_comment\")\n",
    "def analyze_comment(text: str):\n",
    "    problem_keywords = [\"spam\", \"hate\", \"offensive\", \"abuse\"]\n",
    "    \n",
    "    # Convert the input text to lowercase\n",
    "    text_lower = text.lower()\n",
    "    # Extract matching flags using list comprehension\n",
    "    found_issues = [keyword for keyword in problem_keywords if keyword in text_lower]\n",
    "    # Return the dictionary with required keys\n",
    "    return {\n",
    "        \"issues\": found_issues,\n",
    "        \"issue_count\": len(found_issues),\n",
    "        \"original_text\": text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d5310d3-52f7-42f1-8e2e-49fad6228d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'issues': ['spam', 'hate'], 'issue_count': 2, 'original_text': 'This is a spam with spam which I hate and abusive message'}\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    \"http://localhost:8000/analyze_comment?text=This is a spam with spam which I hate and abusive message\"\n",
    ")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147fe48-93b4-4b41-a364-6bf8e6c7aa3d",
   "metadata": {},
   "source": [
    "#### Handle numerical request data\n",
    "\n",
    "You're building a content moderation system. The system needs to calculate a trust score for each user comment based on numerical features - `length`, `user_reputation`, and `report_count`. You'll create an endpoint that processes these features to make them compatible for the moderation model.\n",
    "\n",
    "Note that the ML model and `CommentMetrics Pydantic model with `length`(int), `user_reputation`(int) and `report_count`(int) are already created and loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce1f4f3b-5c61-49d6-8f95-4f35f42a7151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scorer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scorer.py\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class CommentMetrics(BaseModel):\n",
    "    length: int\n",
    "    user_reputation: int\n",
    "    report_count: int\n",
    "\n",
    "class CommentScorer:\n",
    "    def predict(self, features: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Predict trust score based on comment metrics\n",
    "        features: [[length, user_reputation, report_count]]\n",
    "        \"\"\"\n",
    "        # Unpack features\n",
    "        length, reputation, reports = features[0]\n",
    "        \n",
    "        # Calculate trust score\n",
    "        score = (0.3 * (length/500) +        # Normalize length\n",
    "                 0.5 * (reputation/100) +    # Normalize reputation\n",
    "                 -0.2 * reports)             # Reports reduce score\n",
    "        \n",
    "        return float(max(min(score * 100, 100), 0))  # Scale to 0-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "294fd071-3eca-490c-9f89-265e9c50066e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import numpy as np\n",
    "from scorer import CommentMetrics, CommentScorer\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "model = CommentScorer()\n",
    "\n",
    "@app.post(\"/predict_trust\")\n",
    "def predict_trust(comment: CommentMetrics):\n",
    "    # Convert input and extract comment metrics\n",
    "    features = np.array([[\n",
    "        comment.length,\n",
    "        comment.user_reputation,\n",
    "        comment.report_count\n",
    "    ]])\n",
    "    # Get prediction from model \n",
    "    score = model.predict(features)\n",
    "    return {\n",
    "        \"trust_score\": round(score, 2),\n",
    "        \"comment_metrics\": comment.dict()\n",
    "    }\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8fc44ca-d68f-4902-992b-ef81ed32ffde",
   "metadata": {},
   "source": [
    "curl -X POST \"http://localhost:8080/predict_trust\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "           \"length\": 150,\n",
    "           \"user_reputation\": 100,\n",
    "           \"report_count\": 0\n",
    "         }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e431c89-cc86-4f28-ba41-addd24825300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trust_score': 59.0, 'comment_metrics': {'length': 150, 'user_reputation': 100, 'report_count': 0}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/predict_trust\"\n",
    "data = {\n",
    "    \"length\": 150,\n",
    "    \"user_reputation\": 100,\n",
    "    \"report_count\": 0}\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c842068-ab13-4ba8-991d-4ef10813e102",
   "metadata": {},
   "source": [
    "### Section 2.2 - Input validation in PastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184207a5-b6b7-41d1-9d4e-66a36f948936",
   "metadata": {},
   "source": [
    "#### Field validation\n",
    "\n",
    "You are building on the user comment moderation service. Your goal is to create a Pydantic `User` model that ensures data integrity across all users.\n",
    "\n",
    "Implement validations for the `username` (min 5, max 50 characters) field.\n",
    "\n",
    "Use Pydantic's `Field` class to add these constraints, and test your model with both valid and invalid product data to ensure it correctly handles various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d71195c-5fe4-45aa-9ed4-e916ee5d95ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username='john_doe' email='john@mode360.com' age=25\n"
     ]
    }
   ],
   "source": [
    "# Import the base model and field validator from Pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Inherit Pydantic's base model\n",
    "class User(BaseModel): \n",
    "    # Set minimum and maximum name length\n",
    "    username: str = Field(..., min_length=5, max_length=20)\n",
    "    email: str\n",
    "    age: int\n",
    "\n",
    "user = User(username=\"john_doe\", email=\"john@mode360.com\", age=25)\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e66e7-195b-4228-9066-06eca7c72272",
   "metadata": {},
   "source": [
    "#### Adding custom validators\n",
    "\n",
    "Mode360 Solutions is the organization behind the comment moderation system, and you're given a task to create a validation service for all employees. The system should be able to validate input details (`username`, `email` and `age`) such that only employees with an official email address can register.\n",
    "\n",
    "You need to define a Pydantic `User` model using `@validator` decorator on `email` to check if the entered email ends with `@mode360.com`\n",
    "\n",
    "These validators enhance security and system integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e42c837e-0ca9-445a-885b-b1e98b0f3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator, Field\n",
    "\n",
    "class User(BaseModel):\n",
    "    username: str = Field(..., min_length=5, max_length=20)  \n",
    "    email: str\n",
    "    age: int\n",
    "\n",
    "    # Add the Pydantic decorator to validate\n",
    "    @field_validator('email')  \n",
    "    def email_must_be_example_domain(cls, user_email):\n",
    "        # Use the endswith method to validate the email ends with @mode360.com\n",
    "        if not user_email.endswith(\"@mode360.com\"):\n",
    "            raise ValueError('Email must be from the mode360.com domain')\n",
    "        return user_email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd0c7c-3df3-4252-b5f0-114a7aca3654",
   "metadata": {},
   "source": [
    "#### Testing custom validators\n",
    "\n",
    "After defining custom validators, you need to add that validator to the API endpoint and ensure it is working as expected.\n",
    "\n",
    "The system should ensure users with valid email addresses are able to register. Here you need to create a simple endpoint that expects user details(`username`, `email` and `age`) in the request. You need to add the pydantic model to the endpoint and test the endpoint for invalid email address using `cURL` command, provided in the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e58f50c-1f10-4fe1-aa77-33d72cf9bb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class User(BaseModel):\n",
    "    username: str = Field(..., min_length=3, max_length=50)\n",
    "    email: str\n",
    "    age: int\n",
    "    \n",
    "    @validator('email')\n",
    "    def email_must_be_example_domain(cls, user_email):\n",
    "        if not user_email.endswith(\"@mode360.com\"):\n",
    "            raise ValueError('Email must be from the mode360.com domain')\n",
    "        return user_email\n",
    "\n",
    "# Create a post request endpoint\n",
    "@app.post(\"/register\")\n",
    "# Validate incoming user data with a pydantic model\n",
    "def register_user(user: User):\n",
    "    return {\"status\": \"success\", \"user\": user.dict()}\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b939e00c-1e21-4c65-a420-06630ef72945",
   "metadata": {},
   "source": [
    "curl -X POST \"http://localhost:8080/register\" -H \"Content-Type: application/json\" -d '{\"username\": \"jane_doe\", \"email\": \"jane@mode360.com\", \"age\": 30}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f8446-2479-498a-8de3-94464af2775f",
   "metadata": {},
   "source": [
    "**remark** - play with the `data` below and violate validation restrictions to see the validation fails and provide errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8465949-e3bc-43a4-928f-2d9e48e49e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'success', 'user': {'username': 'jane_doe', 'email': 'jane@mode360.com', 'age': 30}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/register\"\n",
    "data = {\n",
    "    \"username\": \"jane_doe\", \n",
    "    \"email\": \"jane@mode360.com\", \n",
    "    \"age\": 30}\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393ce6f-0160-447f-9453-23e3e2e97fab",
   "metadata": {},
   "source": [
    "### Section 2.3 - Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44babe80-9162-4dab-a633-e9ca0cf109b7",
   "metadata": {},
   "source": [
    "#### Loading AI model at server startup\n",
    "\n",
    "You have to deploy a trained sentiment analysis model that helps in moderating comments from users. To ensure zero downtime, the API needs to be ready to analyze user comments as soon as it starts up.\n",
    "\n",
    "In this exercise, you'll implement FastAPI's lifespan events to load your model efficiently to build the comment moderation systems. The `SentimentAnalyzer` model class is already defined and imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f0fbfe2-2706-47ec-8d90-fcf860291c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the context manager decorator from contextlib module\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "sentiment_model = None\n",
    "\n",
    "def load_model():\n",
    "    global sentiment_model\n",
    "    sentiment_model = SentmentAnalyzer(\"sentiment_model.joblib\")\n",
    "\n",
    "# Use FastAPI's context manager to define lifespan event\n",
    "@asynccontextmanager\n",
    "def lifespan(app: FastAPI):\n",
    "    # Call the function to load the model\n",
    "    load_model()\n",
    "    yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f9426-e886-41b3-a233-316ce56c9a17",
   "metadata": {},
   "source": [
    "#### Health-check API for model loading\n",
    "\n",
    "After loading the model at server startup, you need to develop a testing endpoint to build a monitoring system that can detect if the model is ready to analyze user comments.\n",
    "\n",
    "In this exercise, you'll create a health check endpoint that allows you to verify your API's status and trigger alerts if the `sentiment_model` isn't available.\n",
    "\n",
    "Note: `sentiment_model is already pre-loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "034abd5b-886c-4fae-b1d2-db0bb5b57ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  model.py\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Model creation\n",
    "def train_and_save_model():\n",
    "    data = {\n",
    "        \"review\": [\n",
    "            \"I love this product, it's fantastic!\",\n",
    "            \"Really satisfied with the quality!\",\n",
    "            \"Terrible, I hate it.\",\n",
    "            \"Not happy with the purchase.\",\n",
    "            \"Absolutely amazing and wonderful!\",\n",
    "            \"Worst experience ever.\",\n",
    "            \"I am very pleased with my purchase.\",\n",
    "            \"Disappointed, it didn't work as expected.\",\n",
    "            \"The best thing I've ever bought.\",\n",
    "            \"Totally awful, will not buy again.\"\n",
    "        ],\n",
    "        \"label\": [1, 1, 0, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    positive_words = [\"love\", \"satisfied\", \"amazing\", \"fantastic\", \"wonderful\", \"pleased\", \"best\"]\n",
    "    negative_words = [\"hate\", \"terrible\", \"worst\", \"disappointed\", \"awful\"]\n",
    "\n",
    "    df[\"num_words\"] = df[\"review\"].apply(lambda x: len(x.split()))\n",
    "    df[\"num_positive_words\"] = df[\"review\"].apply(lambda x: sum(word in x.lower() for word in positive_words))\n",
    "    df[\"num_complaints\"] = df[\"review\"].apply(lambda x: sum(word in x.lower() for word in negative_words))\n",
    "\n",
    "    X = df[[\"num_words\", \"num_positive_words\", \"num_complaints\"]]\n",
    "    y = df[\"label\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save model using joblib instead of pickle\n",
    "    joblib.dump(model, 'sentiment_model.joblib', compress=3)\n",
    "\n",
    "# Define a callable class\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_path):\n",
    "        # Load the model using joblib\n",
    "        train_and_save_model()\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.positive_words = [\"love\", \"satisfied\", \"amazing\", \"fantastic\", \"wonderful\", \"pleased\", \"best\"]\n",
    "        self.negative_words = [\"hate\", \"terrible\", \"worst\", \"disappointed\", \"awful\"]\n",
    "\n",
    "    def __call__(self, text):\n",
    "        num_words = len(text.split())\n",
    "        num_positive_words = sum(word in text.lower() for word in self.positive_words)\n",
    "        num_complaints = sum(word in text.lower() for word in self.negative_words)\n",
    "        features = [[num_words, num_positive_words, num_complaints]]\n",
    "        \n",
    "        # Get prediction and confidence score\n",
    "        prediction = self.model.predict(features)\n",
    "        confidence_scores = self.model.predict_proba(features)\n",
    "        \n",
    "        # Return dictionary directly instead of JSON string\n",
    "        result = {\n",
    "            \"label\": \"Positive\" if prediction[0] == 1 else \"Negative\",\n",
    "            \"confidence\": float(confidence_scores[0][prediction[0]])  \n",
    "        }\n",
    "        return result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b985704-cdfe-429b-9a71-6ae071ea3282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from model import SentimentAnalyzer\n",
    "from fastapi import FastAPI\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "def load_model():\n",
    "    global sentiment_model\n",
    "    sentiment_model = SentimentAnalyzer(\"sentiment_model.joblib\")\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    load_model()\n",
    "    yield\n",
    "\n",
    "app = FastAPI(title=\"Sentiment Analysis API\", lifespan=lifespan)\n",
    "\n",
    "# Define a GET endpoint at route \"/health\"\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "  \t# Check whether sentiment_model is loaded or not.\n",
    "    if sentiment_model is not None:\n",
    "        return {\n",
    "          \t# Mark status as healthy and loaded boolean to True\n",
    "            \"status\": \"healthy\",\n",
    "            \"model_loaded\": True\n",
    "        }\n",
    "    # Mark status as unhealthy and loaded boolean to False\n",
    "    return {\n",
    "        \"status\": \"unhealthy\",\n",
    "        \"model_loaded\": False\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3a6d06b-216b-4a00-a3cc-a337417f3fbf",
   "metadata": {},
   "source": [
    "curl -X GET \"http://localhost:8080/health\" -H \"accept: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362d6a46-9dbc-4ff1-a206-56ca7744278a",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/health\"\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884f9b6-402a-463f-995f-887fd3e25722",
   "metadata": {},
   "source": [
    "### Section 2.3 - Returning structured predictions resposne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aaec6e-dddf-4a1b-a922-fae1d6ed4f00",
   "metadata": {},
   "source": [
    "#### Returning structured output from API\n",
    "\n",
    "You're building a content moderation system where you need to define a POST endpoint to test a pre-trained sentiment analysis model on user comments.\n",
    "\n",
    "You need to create an endpoint that leverages `pydantic` models to return predictions in a structured format.\n",
    "\n",
    "Note: Pydantic models - `CommentRequest` and `CommentResponse` are already created for you to use along with the pre-trained `sentiment_model` from pre-defined `SentimentAnalyzer` class."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8f452b5-2bee-48db-b7e3-6074180f6521",
   "metadata": {},
   "source": [
    "# Create a POST request endpoint\n",
    "@app.post(\"/analyze\")\n",
    "# Capture the request text for validation as per CommentRequest model\n",
    "def analyze_comment(request: CommentRequest):\n",
    "    try:\n",
    "        # Specify pass the request text to the model\n",
    "        result = sentiment_model(CommentRequest.text)\n",
    "        # Specify the result attributes to complete the comment response\n",
    "        return CommentResponse(text=request.text, \n",
    "                               sentiment=result[0][\"label\"], \n",
    "                               confidence=result[0][\"score\"])\n",
    "    except Exception:\n",
    "        raise HTTPException(status_code=500,\n",
    "            detail=\"Prediction failed\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626fe31-ef6a-41ac-9696-52db048d25d0",
   "metadata": {},
   "source": [
    "#### Testing the endpoint for structured response\n",
    "Now that the `analyze_comment` endpoint is created, you need to test it using Python's requests library.\n",
    "\n",
    "You'll create a script to send `POST` request containing sample text to your API and handle the responses.\n",
    "\n",
    "Note: the API code is already compiled and captured in `api.py` script. For this exercise, you will be working in `main.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6df0ed21-cfe5-44f5-a130-e47e6d370a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing api.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api.py\n",
    "# File: sentiment_api.py\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from contextlib import asynccontextmanager\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# model creation\n",
    "def train_and_save_model():\n",
    "    data = {\n",
    "        \"review\": [\n",
    "            \"I love this product, it's fantastic!\",\n",
    "            \"Really satisfied with the quality!\",\n",
    "            \"Terrible, I hate it.\",\n",
    "            \"Not happy with the purchase.\",\n",
    "            \"Absolutely amazing and wonderful!\",\n",
    "            \"Worst experience ever.\",\n",
    "            \"I am very pleased with my purchase.\",\n",
    "            \"Disappointed, it didn't work as expected.\",\n",
    "            \"The best thing I've ever bought.\",\n",
    "            \"Totally awful, will not buy again.\"\n",
    "        ],\n",
    "        \"label\": [1, 1, 0, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    positive_words = [\"love\", \"satisfied\", \"amazing\", \"fantastic\", \"wonderful\", \"pleased\", \"best\"]\n",
    "    negative_words = [\"hate\", \"terrible\", \"worst\", \"disappointed\", \"awful\"]\n",
    "\n",
    "    df[\"num_words\"] = df[\"review\"].apply(lambda x: len(x.split()))\n",
    "    df[\"num_positive_words\"] = df[\"review\"].apply(lambda x: sum(word in x.lower() for word in positive_words))\n",
    "    df[\"num_complaints\"] = df[\"review\"].apply(lambda x: sum(word in x.lower() for word in negative_words))\n",
    "\n",
    "    X = df[[\"num_words\", \"num_positive_words\", \"num_complaints\"]]\n",
    "    y = df[\"label\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save model using joblib instead of pickle\n",
    "    joblib.dump(model, 'sentiment_model.joblib', compress=3)\n",
    "\n",
    "# Train and save the model\n",
    "train_and_save_model()\n",
    "\n",
    "sentiment_model = None\n",
    "\n",
    "# Define a callable class\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_path):\n",
    "        # Load the model using joblib\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.positive_words = [\"love\", \"satisfied\", \"amazing\", \"fantastic\", \"wonderful\", \"pleased\", \"best\"]\n",
    "        self.negative_words = [\"hate\", \"terrible\", \"worst\", \"disappointed\", \"awful\"]\n",
    "\n",
    "    def __call__(self, text):\n",
    "        num_words = len(text.split())\n",
    "        num_positive_words = sum(word in text.lower() for word in self.positive_words)\n",
    "        num_complaints = sum(word in text.lower() for word in self.negative_words)\n",
    "        features = [[num_words, num_positive_words, num_complaints]]\n",
    "        \n",
    "        # Get prediction and confidence score\n",
    "        prediction = self.model.predict(features)\n",
    "        confidence_scores = self.model.predict_proba(features)\n",
    "        \n",
    "        # Return dictionary directly instead of JSON string\n",
    "        result = {\n",
    "            \"label\": \"Positive\" if prediction[0] == 1 else \"Negative\",\n",
    "            \"confidence\": float(confidence_scores[0][prediction[0]])  \n",
    "        }\n",
    "        return result \n",
    "\n",
    "# Define model loading function\n",
    "def load_model():\n",
    "    global sentiment_model\n",
    "    sentiment_model = SentimentAnalyzer(\"sentiment_model.joblib\")\n",
    "\n",
    "# Initialize FastAPI app with lifespan\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    # Initialize our mock model on startup\n",
    "    global sentiment_model\n",
    "    load_model()\n",
    "    yield\n",
    "\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"Sentiment Analysis API\", lifespan=lifespan)\n",
    "\n",
    "# Initialize model variable\n",
    "sentiment_model = None\n",
    "\n",
    "# Define request/response models\n",
    "class CommentRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class CommentResponse(BaseModel):\n",
    "    text: str\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "    \n",
    "\n",
    "@app.post(\"/analyze\")\n",
    "def analyze_comment(request: CommentRequest):\n",
    "    if sentiment_model is None:\n",
    "        raise HTTPException(\n",
    "            status_code=503,\n",
    "            detail=\"Model not loaded\"\n",
    "        )\n",
    "    \n",
    "    if not request.text.strip():\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"Empty text provided\"\n",
    "        )\n",
    "        \n",
    "    result = sentiment_model(request.text)\n",
    "    return CommentResponse(\n",
    "        text=request.text,\n",
    "        sentiment=result[\"label\"],\n",
    "        confidence=result[\"confidence\"]\n",
    "    )\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    \"\"\"Check if model is loaded and ready\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\" if sentiment_model is not None else \"unhealthy\",\n",
    "        \"model_loaded\": sentiment_model is not None\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb933cc7-f271-4544-ae49-70cc9edc005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'This is great, I can totally relate.', 'sentiment': 'Negative', 'confidence': 0.6244529524435871}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/analyze\"\n",
    "data = {\"text\": \"This is great, I can totally relate.\"}\n",
    "\n",
    "# Send post request and pass the sample request data\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print prediction response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2cda1-68a7-41d9-85be-ec4889d1e1d7",
   "metadata": {},
   "source": [
    "## Chapter 3 - API Key Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805d07f-4706-4072-9dc7-487f0ef681ec",
   "metadata": {},
   "source": [
    "### Securing APIs with key authentication\n",
    "\n",
    "You're building a secure API and need to implement API key verification. The API will check for a key in the `X-API-Key` header of each request and verify it against a predefined secret. You'll use FastAPI's built-in security features to implement this authentication system.\n",
    "\n",
    "The `FastAPI` and `HTTPException` classes have been pre-imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44d42d32-40e4-4e53-b6cb-d24fc5893cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function that handles dependencies\n",
    "from fastapi import Depends, FastAPI, HTTPException\n",
    "from fastapi.security import APIKeyHeader\n",
    "\n",
    "# Create the API key instance\n",
    "api_key_header = APIKeyHeader(name=\"X-API-Key\")\n",
    "API_KEY = \"your_secret_key\"\n",
    "\n",
    "# Pass the APIKeyHeader instance and verify against input api_key\n",
    "def verify_api_key(api_key: str = Depends(api_key_header)):\n",
    "    if api_key != API_KEY:  \n",
    "      \t# Raise the HTTP exception here\n",
    "        raise HTTPException(status_code=403, detail=\"Invalid API Key\")  \n",
    "    return api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a95024-57ad-4d5a-87a6-c6c39161e6f0",
   "metadata": {},
   "source": [
    "#### Secure the API endpoint\n",
    "\n",
    "You have a sentiment analysis model API that needs to be protected with API key authentication.\n",
    "\n",
    "You've already set up the `verify_api_key` function, and now you need to create a protected endpoint that accepts the request with text input and returns predictions from the model. This endpoint should only be accessible to users with valid API keys.\n",
    "\n",
    "You will be working within `main.py` to complete this exercise.\n",
    "\n",
    "All the supporting code including `verify_api_key` and model loading is in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e3ca5e8f-97e7-4801-bfd9-056ed010bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fastapi import Depends, HTTPException\n",
    "from fastapi.security import APIKeyHeader\n",
    "\n",
    "# Model creation\n",
    "def train_and_save_model():\n",
    "    data = {\n",
    "        \"review\": [\n",
    "            \"I love this product, it's fantastic!\",\n",
    "            \"Really satisfied with the quality!\",\n",
    "            \"Terrible, I hate it.\",\n",
    "            \"Not happy with the purchase.\",\n",
    "            \"Absolutely amazing and wonderful!\",\n",
    "            \"Worst experience ever.\",\n",
    "            \"I am very pleased with my purchase.\",\n",
    "            \"Disappointed, it didn't work as expected.\",\n",
    "            \"The best thing I've ever bought.\",\n",
    "            \"Totally awful, will not buy again.\"\n",
    "        ],\n",
    "        \"label\": [1, 1, 0, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    positive_words = [\"love\", \"satisfied\", \"amazing\", \"fantastic\", \"wonderful\", \"pleased\", \"best\"]\n",
    "    negative_words = [\"hate\", \"terrible\", \"worst\", \"disappointed\", \"awful\"]\n",
    "\n",
    "    df[\"num_words\"] = df[\"review\"].apply(lambda x: len(x.split()))\n",
    "    df[\"num_positive_words\"] = df[\"review\"].apply(lambda x: sum(word in x.lower() for word in positive_words))\n",
    "    df[\"num_complaints\"] = df[\"review\"].apply(lambda x: sum(word in x.lower() for word in negative_words))\n",
    "\n",
    "    X = df[[\"num_words\", \"num_positive_words\", \"num_complaints\"]]\n",
    "    y = df[\"label\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save model using joblib instead of pickle\n",
    "    joblib.dump(model, 'sentiment_model.joblib', compress=3)\n",
    "\n",
    "# Define a callable class\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_path):\n",
    "        # Load the model using joblib\n",
    "        train_and_save_model()\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.positive_words = [\"love\", \"satisfied\", \"amazing\", \"fantastic\", \"wonderful\", \"pleased\", \"best\"]\n",
    "        self.negative_words = [\"hate\", \"terrible\", \"worst\", \"disappointed\", \"awful\"]\n",
    "\n",
    "    def __call__(self, text):\n",
    "        num_words = len(text.split())\n",
    "        num_positive_words = sum(word in text.lower() for word in self.positive_words)\n",
    "        num_complaints = sum(word in text.lower() for word in self.negative_words)\n",
    "        features = [[num_words, num_positive_words, num_complaints]]\n",
    "        \n",
    "        # Get prediction and confidence score\n",
    "        prediction = self.model.predict(features)\n",
    "        confidence_scores = self.model.predict_proba(features)\n",
    "        \n",
    "        # Return dictionary directly instead of JSON string\n",
    "        result = {\n",
    "            \"label\": \"Positive\" if prediction[0] == 1 else \"Negative\",\n",
    "            \"confidence\": float(confidence_scores[0][prediction[0]])  \n",
    "        }\n",
    "        return result \n",
    "      \n",
    "      \n",
    "api_key_header = APIKeyHeader(name=\"X-API-Key\")\n",
    "API_KEY = \"your_secret_key\"\n",
    "\n",
    "# Pass the variable containing the APIKeyHeader\n",
    "def verify_api_key(api_key: str = Depends(api_key_header)):  \n",
    "    # Verify the API key\n",
    "    if api_key != API_KEY:  \n",
    "      \t# Raise the HTTP exception here\n",
    "        raise HTTPException(status_code=403, detail=\"Invalid API Key\")\n",
    "    return api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "72fbdf7c-0e4d-4c59-bdaa-a6b92d37e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from model import SentimentAnalyzer, verify_api_key\n",
    "from fastapi import FastAPI, Depends\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class SentimentRequest(BaseModel):\n",
    "    text: str\n",
    "    \n",
    "@app.post(\"/predict\")\n",
    "def get_prediction(\n",
    "    request: SentimentRequest,\n",
    "    # Authenticate the incoming API key using verify_api_key function\n",
    "    api_key: str = Depends(verify_api_key)\n",
    "):\n",
    "    sentiment_model = SentimentAnalyzer(\"sentiment_model.joblib\")\n",
    "    result = sentiment_model(request.text)\n",
    "    return {\n",
    "        \"text\": request.text,\n",
    "        \"sentiment\": result,\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5d6c6a7-f44f-434e-b647-2cb9104406a8",
   "metadata": {},
   "source": [
    "curl -X POST \\\n",
    "  http://localhost:8080/predict \\\n",
    "  -H \"X-API-Key: your_secret_key\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"text\": \"This is not a good product\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cb1b14fc-0cee-453b-93cb-7f6c800841df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'This is not a good product', 'sentiment': {'label': 'Negative', 'confidence': 0.6826307822443081}, 'status': 'success'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/predict\" \n",
    "headers = {\"X-API-Key\": \"your_secret_key\", \"Content-Type\": \"application/json\"}\n",
    "data = {\"text\": \"This is not a good product\"}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "654b42fd-961b-4052-9c40-19e8c64f3ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detail': 'Invalid API Key'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/predict\" \n",
    "headers = {\"X-API-Key\": \"wrong_key\", \"Content-Type\": \"application/json\"}\n",
    "data = {\"text\": \"This is not a good product\"}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326a173-b43e-49bf-b40a-532e8c772cf5",
   "metadata": {},
   "source": [
    "#### Implementing rate limiter\n",
    "\n",
    "You're building a sentiment analysis API where users can analyze texts for sentiments. To prevent abuse, you need to implement rate limiting that allows only `5` requests per minute per API key. The `RateLimiter` class is already created and you have to add the `is_rate_limited` method within the `RateLimiter` class that checks the number of requests that have been made within the 1 minute time window.\n",
    "\n",
    "The `datetime` and `timedelta` classes from the datetime library have been pre-imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ebd82e15-0225-4fa4-927b-cfaa638d4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a735f2d2-209e-490c-8b82-266975ce2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rate_limited(self, api_key: str) -> bool:\n",
    "    # Get current time and the timestamp for one minute ago\n",
    "    now = datetime.now()\n",
    "    minute_ago = now - timedelta(minutes=1)\n",
    "    \n",
    "    # Remove requests older than 1 minute\n",
    "    self.requests[api_key] = [\n",
    "        req_time for req_time in self.requests[api_key]\n",
    "        if req_time > minute_ago]\n",
    "    \n",
    "    # Check if no. of requests exceeded the set limit\n",
    "    if len(self.requests[api_key]) > self.requests_per_minute:\n",
    "        return True\n",
    "    self.requests[api_key].append(now)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92b647-c387-4dcc-9341-ea840a5e6355",
   "metadata": {},
   "source": [
    "#### Add rate limiting to endpoint\n",
    "\n",
    "The next step in securing the API endpoint is to add rate limit logic to the previously defined `test_api_key` function that checks both the API key validity and enforces rate limiting. You need to integrate the rate limiting logic that you defined in the `RateLimiter` class and raise HTTP exceptions in case the limit is exceeded.\n",
    "\n",
    "You will be updating the `test_api_key` function from `main.py` script here.\n",
    "\n",
    "All the supporting code for model creation and loading is in `model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a5bd6fdc-347c-4510-a9e2-7c661fe99545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from fastapi.security import APIKeyHeader\n",
    "\n",
    "api_key_header = APIKeyHeader(name=\"X-API-Key\")\n",
    "API_KEY = \"your_secret_key\"\n",
    "\n",
    "# Model creation\n",
    "def train_and_save_model():\n",
    "    data = {\n",
    "        \"review\": [\n",
    "            \"I love this product, it's fantastic!\",\n",
    "            \"Really satisfied with the quality!\",\n",
    "            \"Terrible, I hate it.\",\n",
    "            \"Not happy with the purchase.\",\n",
    "            \"Absolutely amazing and wonderful!\",\n",
    "            \"Worst experience ever.\",\n",
    "            \"I am very pleased with my purchase.\",\n",
    "            \"Disappointed, it didn't work as expected.\",\n",
    "            \"The best thing I've ever bought.\",\n",
    "            \"Totally awful, will not buy again.\"\n",
    "        ],\n",
    "        \"label\": [1, 1, 0, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    positive_words = [\"love\", \"satisfied\", \"amazing\", \"fantastic\", \"wonderful\", \"pleased\", \"best\"]\n",
    "    negative_words = [\"hate\", \"terrible\", \"worst\", \"disappointed\", \"awful\"]\n",
    "\n",
    "    df[\"num_words\"] = df[\"review\"].apply(lambda x: len(x.split()))\n",
    "    df[\"num_positive_words\"] = df[\"review\"].apply(lambda x: sum(word in x.lower() for word in positive_words))\n",
    "    df[\"num_complaints\"] = df[\"review\"].apply(lambda x: sum(word in x.lower() for word in negative_words))\n",
    "\n",
    "    X = df[[\"num_words\", \"num_positive_words\", \"num_complaints\"]]\n",
    "    y = df[\"label\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save model using joblib instead of pickle\n",
    "    joblib.dump(model, 'sentiment_model.joblib', compress=3)\n",
    "\n",
    "# Define a callable class\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_path):\n",
    "        # Load the model using joblib\n",
    "        train_and_save_model()\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.positive_words = [\"love\", \"satisfied\", \"amazing\", \"fantastic\", \"wonderful\", \"pleased\", \"best\"]\n",
    "        self.negative_words = [\"hate\", \"terrible\", \"worst\", \"disappointed\", \"awful\"]\n",
    "\n",
    "    def __call__(self, text):\n",
    "        num_words = len(text.split())\n",
    "        num_positive_words = sum(word in text.lower() for word in self.positive_words)\n",
    "        num_complaints = sum(word in text.lower() for word in self.negative_words)\n",
    "        features = [[num_words, num_positive_words, num_complaints]]\n",
    "        \n",
    "        # Get prediction and confidence score\n",
    "        prediction = self.model.predict(features)\n",
    "        confidence_scores = self.model.predict_proba(features)\n",
    "        \n",
    "        # Return dictionary directly instead of JSON string\n",
    "        result = {\n",
    "            \"label\": \"Positive\" if prediction[0] == 1 else \"Negative\",\n",
    "            \"confidence\": float(confidence_scores[0][prediction[0]])  \n",
    "        }\n",
    "        return result \n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, requests_per_minute: int = 10):\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.requests = defaultdict(list)  # Store request timestamps per API key\n",
    "\n",
    "    def is_rate_limited(self, api_key: str) -> tuple[bool, int]:\n",
    "        \"\"\"\n",
    "        Check if the request should be rate limited\n",
    "        Returns (is_limited, requests_remaining)\n",
    "        \"\"\"\n",
    "        now = datetime.now()\n",
    "        minute_ago = now - timedelta(minutes=1)\n",
    "        \n",
    "        # Remove requests older than 1 minute\n",
    "        self.requests[api_key] = [\n",
    "            req_time for req_time in self.requests[api_key]\n",
    "            if req_time > minute_ago\n",
    "        ]\n",
    "        \n",
    "        # Check if rate limit is exceeded\n",
    "        recent_requests = len(self.requests[api_key])\n",
    "        if recent_requests >= self.requests_per_minute:\n",
    "            return True, 0\n",
    "            \n",
    "        # Add new request timestamp\n",
    "        self.requests[api_key].append(now)\n",
    "        return False, self.requests_per_minute - recent_requests - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4615f3f9-4fff-4729-ae31-5e45a971f67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from model import SentimentAnalyzer, RateLimiter, API_KEY, api_key_header\n",
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "rate_limiter = RateLimiter(requests_per_minute=2)\n",
    "\n",
    "def test_api_key(api_key: str = Depends(api_key_header)):\n",
    "    if api_key != API_KEY:\n",
    "        raise HTTPException(\n",
    "            status_code=403, detail=\"Invalid API key\")\n",
    "    \n",
    "    # Check rate limit corresponding to the input api key\n",
    "    is_limited, _ = rate_limiter.is_rate_limited(API_KEY)\n",
    "    # Check if returned boolean by is_rate_limited is True\n",
    "    if is_limited:\n",
    "        # Raise the http exception with status code\n",
    "        raise HTTPException(status_code=429,\n",
    "            detail=\"Rate limit exceeded. Please try again later.\"\n",
    "        )\n",
    "    return api_key\n",
    "\n",
    "class SentimentRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def get_prediction(\n",
    "    request: SentimentRequest,\n",
    "    api_key: str = Depends(test_api_key)\n",
    "):\n",
    "    sentiment_model = SentimentAnalyzer(\"sentiment_model.joblib\")\n",
    "    result = sentiment_model(request.text)\n",
    "    return {\n",
    "        \"text\": request.text,\n",
    "        \"sentiment\": result,\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e7551cc-47d2-4195-a3ba-26f01c3700d4",
   "metadata": {},
   "source": [
    "curl -X POST \\\n",
    "  http://localhost:8080/predict \\\n",
    "  -H \"X-API-Key: your_secret_key\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"text\": \"This is not a good product\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a88b02-c629-40cf-871e-3b66cda0c939",
   "metadata": {},
   "source": [
    "Curl it three times to see we exceeded the rate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "226c0228-3599-45d2-a716-14ea3e6879c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"detail\":\"Rate limit exceeded. Please try again later.\"}"
     ]
    }
   ],
   "source": [
    "!curl -X POST  http://localhost:8080/predict -H \"X-API-Key: your_secret_key\" -H \"Content-Type: application/json\"  -d '{\"text\": \"This is not a good product\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d3bcb299-d501-4241-bef2-312519c68c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'This is not a good product', 'sentiment': {'label': 'Negative', 'confidence': 0.6826307822443081}, 'status': 'success'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/predict\"\n",
    "headers = { \"X-API-Key\": \"your_secret_key\",\n",
    "            \"Content-Type\": \"application/json\"}\n",
    "data = {\"text\": \"This is not a good product\"}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46ab2d-edd5-47af-8278-e9f329c6b90b",
   "metadata": {},
   "source": [
    "### Section 3.3 - Asynchonous processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acfb8f7-0390-4d9f-aa05-92aaf6c84595",
   "metadata": {},
   "source": [
    "#### Create an async sentiment analysis endpoint\n",
    "\n",
    "You're building a social media analytics platform that needs to analyze reviews for sentiment. To handle high traffic efficiently, you need to implement an `async` endpoint. The sentiment analysis model is already loaded and available as sentiment_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2682dd91-56c3-4d2f-9adf-4edc31ba1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Review(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# Create async endpoint at /analyze route\n",
    "@app.post(\"/analyze\")\n",
    "# Write an asynchronous function to process review's text\n",
    "async def analyze_review(review: Review):\n",
    "    # Run the model in a separate thread to avoid any event loop blockage\n",
    "    result = await asyncio.to_thread(sentiment_model, review.text)\n",
    "    return {\"sentiment\": result[0][\"label\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a3e19-e5ff-4fdd-85d5-c98b7b7facc6",
   "metadata": {},
   "source": [
    "#### Implementing background tasks\n",
    "\n",
    "Your sentiment analysis API is getting requests to process batch of hundreds of reviews at once. To handle this efficiently without making users wait, you'll implement background task processing so that requests are being processed after sending a response to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4428479b-d22a-479d-b8c1-f5ccfb654d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the background task class\n",
    "from fastapi import BackgroundTasks\n",
    "# Create a background task dependency\n",
    "@app.post(\"/analyze_batch\")\n",
    "async def analyze_batch(\n",
    "    reviews: Review,\n",
    "    background_tasks: BackgroundTasks\n",
    "):\n",
    "    async def process_reviews(texts: List[str]):\n",
    "        for text in texts:\n",
    "            result = await asyncio.to_thread(sentiment_model, text)\n",
    "            print(f\"Processed: {result[0]['label']}\")\n",
    "    # Add the task of analysing reviews' texts to the background\n",
    "    background_tasks.add_task(process_reviews, reviews.texts)\n",
    "    return {\"message\": \"Processing started\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219cc95-2b7d-41dc-8007-38efff2b2311",
   "metadata": {},
   "source": [
    "#### Handling timeout errors\n",
    "\n",
    "To make the review analysis system more robust and user-friendly, you are now required to add error handling to the analyze_review endpoint. You need to handle timeouts and internal server errors if the model couldn't process the text in time or if there was an error while making the predictions.\n",
    "\n",
    "To simulate this test, a delay of 10 seconds has been added in the model prediction process in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f5b22dfb-8521-4333-8dfb-81d71b94a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import asyncio\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Model creation\n",
    "def train_and_save_model():\n",
    "    data = {\n",
    "        \"review\": [\n",
    "            \"I love this product, it's fantastic!\",\n",
    "            \"Really satisfied with the quality!\",\n",
    "            \"Terrible, I hate it.\",\n",
    "            \"Not happy with the purchase.\",\n",
    "            \"Absolutely amazing and wonderful!\",\n",
    "            \"Worst experience ever.\",\n",
    "            \"I am very pleased with my purchase.\",\n",
    "            \"Disappointed, it didn't work as expected.\",\n",
    "            \"The best thing I've ever bought.\",\n",
    "            \"Totally awful, will not buy again.\"\n",
    "        ],\n",
    "        \"label\": [1, 1, 0, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    positive_words = [\"love\", \"satisfied\", \"amazing\", \"fantastic\", \"wonderful\", \"pleased\", \"best\"]\n",
    "    negative_words = [\"hate\", \"terrible\", \"worst\", \"disappointed\", \"awful\"]\n",
    "\n",
    "    df[\"num_words\"] = df[\"review\"].apply(lambda x: len(x.split()))\n",
    "    df[\"num_positive_words\"] = df[\"review\"].apply(lambda x: sum(word in x.lower() for word in positive_words))\n",
    "    df[\"num_complaints\"] = df[\"review\"].apply(lambda x: sum(word in x.lower() for word in negative_words))\n",
    "\n",
    "    X = df[[\"num_words\", \"num_positive_words\", \"num_complaints\"]]\n",
    "    y = df[\"label\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    model = LogisticRegression(solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save model using joblib instead of pickle\n",
    "    joblib.dump(model, 'sentiment_model.joblib', compress=3)\n",
    "\n",
    "# Define a callable class\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Load the model using joblib\n",
    "        train_and_save_model()\n",
    "        self.model = joblib.load(\"sentiment_model.joblib\")\n",
    "        self.positive_words = [\"love\", \"satisfied\", \"amazing\", \"fantastic\", \"wonderful\", \"pleased\", \"best\"]\n",
    "        self.negative_words = [\"hate\", \"terrible\", \"worst\", \"disappointed\", \"awful\"]\n",
    "\n",
    "    async def __call__(self, text):\n",
    "        # Simulate a long-running operation\n",
    "        await asyncio.sleep(11)  # This will trigger the timeout since it's > 10 seconds\n",
    "        \n",
    "        num_words = len(text.split())\n",
    "        num_positive_words = sum(word in text.lower() for word in self.positive_words)\n",
    "        num_complaints = sum(word in text.lower() for word in self.negative_words)\n",
    "        features = [[num_words, num_positive_words, num_complaints]]\n",
    "        \n",
    "        # Get prediction and confidence score\n",
    "        prediction = self.model.predict(features)\n",
    "        confidence_scores = self.model.predict_proba(features)\n",
    "        \n",
    "        # Return dictionary directly instead of JSON string\n",
    "        result = {\n",
    "            \"label\": \"Positive\" if prediction[0] == 1 else \"Negative\",\n",
    "            \"confidence\": float(confidence_scores[0][prediction[0]])\n",
    "        }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "45357777-9ff9-4ae3-bffd-83467c9a5ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import asyncio\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from model import SentimentAnalyzer\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Review(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/analyze_reviews\")\n",
    "async def analyze_reviews(review: Review):\n",
    "    try:\n",
    "        sentiment_model = SentimentAnalyzer()\n",
    "        # Set model input and timeout limit\n",
    "        result = await asyncio.wait_for(\n",
    "            sentiment_model(review.text),\n",
    "            timeout=10\n",
    "        )\n",
    "        return {\"sentiment\": result[\"label\"]}      \n",
    "    except asyncio.TimeoutError:\n",
    "        # Raise HTTP status code for timeout error\n",
    "        raise HTTPException(status_code=408, detail=\"Analysis timed out\")\n",
    "    except Exception:\n",
    "        # Raise HTTP status code for internal error\n",
    "        raise HTTPException(status_code=500, detail=\"Analysis failed\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d4e2fb4-4320-4b83-a7e7-5ea6cb489679",
   "metadata": {},
   "source": [
    "curl -X POST \\\n",
    "  http://localhost:8080/analyze_reviews \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"text\": \"This is a test\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7b8b8-6bdd-4c80-bfe1-533f74b5fd87",
   "metadata": {},
   "source": [
    "You can set the sleep in the code shorter for a succesful response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fc256244-ea21-42fc-a596-51bb077322a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"sentiment\":\"Negative\"}"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://localhost:8080/analyze_reviews -H \"Content-Type: application/json\" -d '{\"text\": \"This is a test\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ddcaf3ec-20c1-4c3a-8a9c-8cc32157299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'Negative'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/analyze_reviews\"\n",
    "headers = { \"Content-Type\": \"application/json\"}\n",
    "data = {\"text\": \"This is a test\"}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f3357-fab6-448c-b331-b63e18dafdbf",
   "metadata": {},
   "source": [
    "## Chapter 3 - API Versioning, Monitoring and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f10f75-a45c-4462-88c8-24dd22d97a59",
   "metadata": {},
   "source": [
    "### Section 3.1 - API versioning and documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5779c-5a85-42af-9d0d-716a8620f2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf3bdbf-1d80-4a59-bc67-0c0ffceaf5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ef313-3b2d-47f0-9477-38d9c65f62ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
